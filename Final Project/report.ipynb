{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1af6d0e8-36d2-47c7-88d8-04a46f38ef27",
   "metadata": {},
   "source": [
    "Machine Learning Final Project Report\n",
    "Chang Yan (cyan13) and Jingguo Liang (jliang35)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53e25053-8b82-445f-92ba-0047bd5cfd5e",
   "metadata": {},
   "source": [
    "The goal of this project is to develop deep neural networks that are able to detect human faces in an image and make predictions about this person's age, sex, and race."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60c4afe4-a554-43e2-bbb9-e13310c9fc50",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "The dataset used in the project is UTKFace, which can be found on the following website:\n",
    "https://susanqq.github.io/UTKFace/\n",
    "\n",
    "The raw data consists of four parts: \n",
    "(1) an \"original\" RGB image in .jpg format with arbitrary size that contains one human face. \n",
    "\n",
    "(2) a \"cropped\" RGB image in .jpg format with size (200, 200) that is part of the image in (1) where the face locates in. \n",
    "\n",
    "(3) labels for the image, including four parts: age, sex, race of the people in the image, and the date on which the image is collected. \n",
    "The labels are contained in the name of the images, formatted as [age]_[gender]_[race]_[date&time].jpg\n",
    "For cropped images, they are named [age]_[gender]_[race]_[date&time].jpg.chip.jpg. Thus, we can find the corresponding original/cropped images using their labels.\n",
    "[age] is an integer from 0 to 116.\n",
    "[gender] is either 0 (indicating a male) or 1 (indicating a female).\n",
    "[race] is an integer from 0 to 4. 0 denotes White, 1 denotes Black, 2 denotes (East) Asian, 3 denotes Indian, 4 denotes others (e.g. Hispanic, Latino, Middle Eastern)\n",
    "[date&time] is in the format of yyyymmddHHMMSSFFF, indicating the date on which the image is collected to the dataset.\n",
    "\n",
    "(4) Facial landmarks located in the cropped image\n",
    "\n",
    "In the project, we will only use the first three parts of the data (original image, cropped image, labels). For the labels, we will only use age, gender and race labels."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2da95465-909c-4d58-9e6f-20776a8f4b3f",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "\n",
    "The first thing we do is to go through all cropped images and remove those which are obvious wrong (clearly not a face). We also removed all black and white images because we want RGB images as input.\n",
    "\n",
    "Then, we want to locate the cropbox in the original image. We can do that through SIFT and then finding the homography that maps the cropped image to the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69c63f-f15a-400c-8b9f-d6755cb91656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def extractKeypoints(img1, img2, N):\n",
    "    sift = cv.SIFT_create()\n",
    "    keypoint1, descriptor1 = sift.detectAndCompute(img1, None)\n",
    "    keypoint2, descriptor2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    if keypoint1 is None or keypoint2 is None or len(keypoint1) < N or len(keypoint2) < N:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    bf = cv.BFMatcher_create()\n",
    "    matches = bf.knnMatch(descriptor1, descriptor2, k = 2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    matches = sorted(good, key = lambda x:x.distance)[:N]\n",
    "\n",
    "    points1 = np.float32([keypoint1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([keypoint2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    return points1, points2\n",
    "\n",
    "[pts1, pts2] = ExtractKeypoints.extractKeypoints(imgOriginal, imgCropped, 25)\n",
    "[H, status] = cv.findHomography(pts2, pts1, method = cv.RANSAC)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9841f28-171b-4d53-9839-017e74b1532b",
   "metadata": {},
   "source": [
    "We notice that some images does not generate any feature through SIFT, or the number of features generated is lower than we want (<25). Those images are discarded.\n",
    "\n",
    "After finding the homography, we can map the four corners of cropbox ([[0, 0], [0, 200], [200, 200], [200, 0]) back to the original image. We notice that after mapping the cropbox back, the sides of the box is not parallel to the side of the image. Thus, we also calculate the circumscribed box of the mapped cropbox.\n",
    "\n",
    "We also discard the boxes which has a significant angle between the original image, because in that case the circumstribed box will be significantly larger than the cropbox. We can do this by extracting the rotation angle from the homography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ce00-35f4-48e6-8113-e5b3edaebdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables\n",
    "\n",
    "angle = abs(math.atan2(H[1,0], H[0,0])) * 180 / math.pi\n",
    "if angle < 5:\n",
    "    originalShape = imgOriginal.shape\n",
    "    croppedShape = imgCropped.shape\n",
    "    croppedPts = np.array([[0, 0, 1], [0, croppedShape[1], 1], [croppedShape[0], croppedShape[1], 1], [croppedShape[0], 0, 1]])\n",
    "    originalPts = np.matmul(H, croppedPts.transpose()).transpose()\n",
    "    originalPts = np.round(originalPts[:, 0:2] / originalPts[:, 2:3]).astype(np.int32)\n",
    "    x1 = np.min(originalPts[:, 0]).clip(0, originalShape[1])\n",
    "    x2 = np.max(originalPts[:, 0]).clip(0, originalShape[1])\n",
    "    y1 = np.min(originalPts[:, 1]).clip(0, originalShape[0])\n",
    "    y2 = np.max(originalPts[:, 1]).clip(0, originalShape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fd59084-e050-4bf3-a954-9ca983dc237d",
   "metadata": {},
   "source": [
    "We would also like to use the intensity image and the edge map in training. Intensity map can be obtained by a weighted combination of RGB values, and the edge map can be obtained through canny edge detection. We can add the intensity image and the edge map to the RGB image, getting a 5-channel image.\n",
    "\n",
    "The input to the neural model should be in nchw format, while currently our image is in hwc format. We can simply swap the axes to get the nchw format that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da3a12-aca1-45ad-9968-332d676019ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables\n",
    "\n",
    "class PreprocessImage(object):\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, img, normalize):\n",
    "        shape = img.shape\n",
    "        intensity = (0.11 * img[:,:,0] + 0.59 * img[:,:,1] + 0.30 * img[:,:,2])\n",
    "        intensity = intensity.reshape((shape[0], shape[1], 1))\n",
    "        edge = cv.Canny(img, 100, 100, apertureSize = 3)\n",
    "        # cv.imshow('img', edge)\n",
    "        # cv.waitKey(0)\n",
    "        edge = edge\n",
    "        edge = edge.reshape((shape[0], shape[1], 1))\n",
    "        if normalize is True:\n",
    "            intensity = intensity / 255.0\n",
    "            edge = edge / 255.0\n",
    "            img = img / 255.0\n",
    "            result = np.append(img, intensity, axis = -1)\n",
    "            result = np.append(result, edge, axis = -1)\n",
    "            result = np.moveaxis(result, -1, 0)\n",
    "            result[[0,2]] = result[[2,0]]\n",
    "            return result.astype(float)\n",
    "        else:\n",
    "            result = np.append(img, intensity, axis = -1)\n",
    "            result = np.append(result, edge, axis = -1)\n",
    "            result = np.moveaxis(result, -1, 0)\n",
    "            result[[0,2]] = result[[2,0]]\n",
    "            return result.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31b393c8-e3ff-4861-8c09-857442daa5c6",
   "metadata": {},
   "source": [
    "The last thing to do is to divide the data into 5 parts. We will use them for 5-fold cross validation in the training. After the division, we can save the data as .npz files, which will be read back and directly used in training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
