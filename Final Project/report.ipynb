{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656b79fe",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project Report\n",
    "Chang Yan (cyan13) and Jingguo Liang (jliang35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61db5e6",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of this project is to develop deep neural networks that are able to detect human faces in an image and make predictions about this person's age, sex, and race. There are lots of applications for this in real-time cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac1e3d",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "We successfully finished all of our deliverables. They are listed below:\n",
    "#### Must accomplish\n",
    "1. A program with a trained deep convolutional network with optimized structure and best tuned parameters, that can predict the age of the face in the input image\n",
    "2. The prediction of the program should be distinctly better than simple benchmark run, like a logistic regression.\n",
    "3. The program should use appropriate data preprocessing mothods to improve the performance.\n",
    "\n",
    "#### Expect to accomplish\n",
    "1. The program should be able to predict not only age but also sex and ethnicity of the face.\n",
    "2. The program should use multiple different network structures, ensembles or other methods to further improve the performance.\n",
    "3. The prediction of the program should reach an accuracy that is usable in real-life, like over 90%,.\n",
    "4. Adding feature extraction methods before the network (like edge detection) to create more features from the image.\n",
    "\n",
    "#### Would like to accomplish\n",
    "1. The program should be able to first identify the location of the face in the image, and then predict using the cropped face area.\n",
    "The position of the face should also be an output.\n",
    "2. The program should be able to take in different sizes of image files.\n",
    "3. The program should be able to identify an image with no face, instead of giving random output.\n",
    "4. The program should work with both RGB and grayscale images, and uses the RGB channels to achieve better performance than\n",
    "grayscale.\n",
    "5. The speed of the prediction should be fast enough to be done in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619587d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in the project is UTKFace, which can be found on the following website:\n",
    "https://susanqq.github.io/UTKFace/\n",
    "\n",
    "The raw data consists of four parts: \n",
    "\n",
    "(1) an \"original\" RGB image in .jpg format with arbitrary size that contains one human face. \n",
    "\n",
    "(2) a \"cropped\" RGB image in .jpg format with size (200, 200) that is part of the image in (1) where the face locates in. \n",
    "\n",
    "(3) labels for the image, including four parts: age, sex, race of the people in the image, and the date on which the image is collected. \n",
    "The labels are contained in the name of the images, formatted as [age]_[gender]_[race]_[date&time].jpg\n",
    "For cropped images, they are named [age]_[gender]_[race]_[date&time].jpg.chip.jpg. Thus, we can find the corresponding original/cropped images using their labels.\n",
    "[age] is an integer from 0 to 116.\n",
    "[gender] is either 0 (indicating a male) or 1 (indicating a female).\n",
    "[race] is an integer from 0 to 4. 0 denotes White, 1 denotes Black, 2 denotes (East) Asian, 3 denotes Indian, 4 denotes others (e.g. Hispanic, Latino, Middle Eastern)\n",
    "[date&time] is in the format of yyyymmddHHMMSSFFF, indicating the date on which the image is collected to the dataset.\n",
    "\n",
    "(4) Facial landmarks located in the cropped image\n",
    "\n",
    "In the project, we will only use the first three parts of the data (original image, cropped image, labels). For the labels, we will only use age, gender and race labels, as data&time is collection time which it irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efed56",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "The full data preprocessing scripts can be found in the repo under the folder \"DataPreprocessing\".\n",
    "### Data cleansing\n",
    "\n",
    "The first thing we do is to go through all cropped images and remove those which are obvious wrong (clearly not a face). We also removed all black and white images because we want RGB images as input.\n",
    "\n",
    "### Crop Box Generation\n",
    "Then, we want to locate the cropbox in the original image. We can do that through SIFT and then finding the homography that maps the cropped image to the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69c63f-f15a-400c-8b9f-d6755cb91656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables (.py files)\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def extractKeypoints(img1, img2, N):\n",
    "    sift = cv.SIFT_create()\n",
    "    keypoint1, descriptor1 = sift.detectAndCompute(img1, None)\n",
    "    keypoint2, descriptor2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    if keypoint1 is None or keypoint2 is None or len(keypoint1) < N or len(keypoint2) < N:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    bf = cv.BFMatcher_create()\n",
    "    matches = bf.knnMatch(descriptor1, descriptor2, k = 2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    matches = sorted(good, key = lambda x:x.distance)[:N]\n",
    "\n",
    "    points1 = np.float32([keypoint1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([keypoint2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    return points1, points2\n",
    "\n",
    "[pts1, pts2] = ExtractKeypoints.extractKeypoints(imgOriginal, imgCropped, 25)\n",
    "[H, status] = cv.findHomography(pts2, pts1, method = cv.RANSAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fb537",
   "metadata": {},
   "source": [
    "We notice that some images does not generate any feature through SIFT, or the number of features generated is lower than we want (<25). Those images are discarded.\n",
    "\n",
    "After finding the homography, we can map the four corners of cropbox ([[0, 0], [0, 200], [200, 200], [200, 0]) back to the original image. We notice that after mapping the cropbox back, the sides of the box is not parallel to the side of the image. Thus, we also calculate the circumscribed box of the mapped cropbox.\n",
    "\n",
    "We also discard the boxes which has a significant angle between the original image, because in that case the circumstribed box will be significantly larger than the cropbox. We can do this by extracting the rotation angle from the homography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4ce00-35f4-48e6-8113-e5b3edaebdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables (.py files)\n",
    "\n",
    "angle = abs(math.atan2(H[1,0], H[0,0])) * 180 / math.pi\n",
    "if angle < 5:\n",
    "    originalShape = imgOriginal.shape\n",
    "    croppedShape = imgCropped.shape\n",
    "    croppedPts = np.array([[0, 0, 1], [0, croppedShape[1], 1], [croppedShape[0], croppedShape[1], 1], [croppedShape[0], 0, 1]])\n",
    "    originalPts = np.matmul(H, croppedPts.transpose()).transpose()\n",
    "    originalPts = np.round(originalPts[:, 0:2] / originalPts[:, 2:3]).astype(np.int32)\n",
    "    x1 = np.min(originalPts[:, 0]).clip(0, originalShape[1])\n",
    "    x2 = np.max(originalPts[:, 0]).clip(0, originalShape[1])\n",
    "    y1 = np.min(originalPts[:, 1]).clip(0, originalShape[0])\n",
    "    y2 = np.max(originalPts[:, 1]).clip(0, originalShape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8b68d",
   "metadata": {},
   "source": [
    "<img src=\"res/SIFT.JPG\"> \n",
    "<center>SIFT</center>\n",
    "\n",
    "<img src=\"res/cropbox.JPG\" width=400 />\n",
    "<center>Cropbox</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0bcb4",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "We would also like to use the intensity image and the edge map in training. Intensity map can be obtained by a weighted combination of RGB values, and the edge map can be obtained through canny edge detection. We can add the intensity image and the edge map to the RGB image, getting a 5-channel image.\n",
    "\n",
    "The input to the neural model should be in [N,C,H,W] format, while currently our image is in [H,W,C] format. We can simply swap the axes to get the nchw format that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da3a12-aca1-45ad-9968-332d676019ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables (.py files)\n",
    "\n",
    "class PreprocessImage(object):\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess(cls, img, normalize):\n",
    "        shape = img.shape\n",
    "        intensity = (0.11 * img[:,:,0] + 0.59 * img[:,:,1] + 0.30 * img[:,:,2])\n",
    "        intensity = intensity.reshape((shape[0], shape[1], 1))\n",
    "        edge = cv.Canny(img, 100, 100, apertureSize = 3)\n",
    "        # cv.imshow('img', edge)\n",
    "        # cv.waitKey(0)\n",
    "        edge = edge\n",
    "        edge = edge.reshape((shape[0], shape[1], 1))\n",
    "        if normalize is True:\n",
    "            intensity = intensity / 255.0\n",
    "            edge = edge / 255.0\n",
    "            img = img / 255.0\n",
    "            result = np.append(img, intensity, axis = -1)\n",
    "            result = np.append(result, edge, axis = -1)\n",
    "            result = np.moveaxis(result, -1, 0)\n",
    "            result[[0,2]] = result[[2,0]]\n",
    "            return result.astype(float)\n",
    "        else:\n",
    "            result = np.append(img, intensity, axis = -1)\n",
    "            result = np.append(result, edge, axis = -1)\n",
    "            result = np.moveaxis(result, -1, 0)\n",
    "            result[[0,2]] = result[[2,0]]\n",
    "            return result.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663455af",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "The original dataset gives ages directly as a number. However, in our model we would like to divide all ages into several categories: <br>\n",
    "0-3: label 0 <br>\n",
    "4-6: label 1 <br>\n",
    "7-10: label 2 <br>\n",
    "11-15: label 3 <br>\n",
    "16-20: label 4 <br>\n",
    "21-25: label 5 <br>\n",
    "26-30: label 6 <br>\n",
    "31-40: label 7 <br>\n",
    "41-50: label 8 <br>\n",
    "51-60: label 9 <br>\n",
    "61-80: label 10 <br>\n",
    "81+: label 11 <br>\n",
    "\n",
    "Besides age, we also have sex and race in the labels. For sex: <br>\n",
    "0: male <br>\n",
    "1: female <br>\n",
    "\n",
    "For race: <br>\n",
    "0: White <br>\n",
    "1: Black <br>\n",
    "2: (East) Asian <br>\n",
    "3: Indian <br>\n",
    "4: Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e006c90",
   "metadata": {},
   "source": [
    "### 5 fold cross validation\n",
    "The last thing to do is to divide the data into 5 parts. We will use them for 5-fold cross validation in the training. After the division, we can save the data as .npz files, which will be read back and directly used in training. Each fold contains 1000 cases. In each fold, there will be one single .npz for all cropboxes, one single .npz for all labels and one .npz file for each of the origina\n",
    "l images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf2e9d",
   "metadata": {},
   "source": [
    "## Model Result Visualization: Facial Detection: Faster-RCNN\n",
    "Here are some examples of the predictions made by the facial detection Faster-RCNN network. The visualization is done using the file \"visualization.py\". Note that this is a fully working prediction file that can use the trained networks to detect face and prediction labels. We also provide a part of the code here for demonstration ONLY. Full code can be found on visualization.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7766506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.savefig(\"image.jpg\")\n",
    "\n",
    "def main(args):\n",
    "    image = cv.imread(\"./Test/9.jpg\")\n",
    "    imageT = read_image(\"./Test/9.jpg\")\n",
    "    image = PreprocessImage.preprocess(image)\n",
    "    shape = image.shape\n",
    "    \n",
    "    imageT = F.resize(imageT, [shape[1], shape[2]])\n",
    "    test_image = image[0:3].astype(float) / 255\n",
    "    model = Rcnn(args.svpath, \"Rcnn_full_feature_fold_4.pt\", 50, 1)\n",
    "    model.load()\n",
    "    boxes, scores = model.predict([test_image])\n",
    "    if (boxes[0] is not None):\n",
    "        result = draw_bounding_boxes(imageT, torch.tensor([boxes[0]], dtype=torch.float), colors=[\"blue\"], width=5)\n",
    "        show(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83885b",
   "metadata": {},
   "source": [
    "<img src=\"res/box1.png\"> \n",
    "<img src=\"res/box2.png\"> \n",
    "<img src=\"res/box3.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ffc7a",
   "metadata": {},
   "source": [
    "We can see that our predication is very accurate. The networks in each fold capture every face in each test set. We only show 3 random cases here, but all other results look as accurate as those, and the accuracy is quite consistent across the 5 folds. This demonstrates that the facial detection of our network is very reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3feba58",
   "metadata": {},
   "source": [
    "## Model Result Visualization: Label Prediction: The Ensemble\n",
    "Here are some examples of the predictions made by the ensemble classification network. The visualization is done using the file \"visualization.py\". Note that this is a fully working prediction file that can use the trained networks to detect face and prediction labels. We also provide a part of the code here for demonstration ONLY. Full code can be found on visualization.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de114024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is only for demonstration purpose and not executable in Jupyter Notebook.\n",
    "# Complete, executable script can be found in the deliverables (.py files)\n",
    "\n",
    "def main(args)\n",
    "    imageCropped = cv.imread('./Test/cropped3.jpg')\n",
    "    imageCropped = PreprocessImage.preprocess2(imageCropped).reshape((1, 5, 224, 224))\n",
    "    model = EnsembleWrapper(args.svpath, \"Ensemble_age_fold_0.pt\", 12, 50, 1)\n",
    "    model.load()\n",
    "    age = model.predict(imageCropped)\n",
    "    print(age)\n",
    "    model = EnsembleWrapper(args.svpath, \"Ensemble_full_sex_fold_4.pt\", 2, 50, 1)\n",
    "    model.load()\n",
    "    sex = model.predict(imageCropped)\n",
    "    print(sex)\n",
    "    model = EnsembleWrapper(args.svpath, \"Ensemble_full_race_fold_4.pt\", 5, 50, 1)\n",
    "    model.load()\n",
    "    race = model.predict(imageCropped)\n",
    "    print(race)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463508ea",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"res/cropped1.JPG\"> \n",
    "<center>\n",
    "    Age: [6] (26-30) <br>\n",
    "    Sex: [0] (Male) <br>\n",
    "    Race: [1] (Black) <br>\n",
    "</center>\n",
    "\n",
    "<img src=\"res/cropped2.JPG\"> \n",
    "<center>\n",
    "    Age: [10] (61-80) <br>\n",
    "    Sex: [0] (Male) <br>\n",
    "    Race: [0] (White) <br>\n",
    "</center>\n",
    "\n",
    "<img src=\"res/cropped3.JPG\"> \n",
    "<center>\n",
    "    Age: [6] (26-30) <br>\n",
    "    Sex: [1] (Female) <br>\n",
    "    Race: [3] (Indian) <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c84c67",
   "metadata": {},
   "source": [
    "We can see that both the prediction of sex and race are quite accurate, but for age the third one is a bit higher than it should be. We believe that there are three reasons for this: 1. The original labels of age may are not accurate, as we actually found some wrongly labeled ones and removed them, but there should still be some missing. 2. Age is very hard to predict even by human eyes, causing difficulties for the network (We also did not use the largest version of each model because of training time). 3. The labels are imbalanced. 26-30 actually is the most common label and the network is a bit biased to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9dce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
