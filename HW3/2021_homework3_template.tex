\documentclass[11pt]{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage[fleqn]{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[many]{tcolorbox}
\usepackage{lipsum}
\usepackage{float}
\usepackage{trimclip}
\usepackage{listings}
\usepackage{environ}% http://ctan.org/pkg/environ
\usepackage{wasysym}
\usepackage{array}


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}
\newcommand{\defeq}{\overset{\text{def}}{=}}
\renewcommand{\vec}[1]{\mathbf{#1}}


\bgroup
\def\arraystretch{1.5}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{z}[1]{>{\centering\arraybackslash}m{#1}}

%Arguments are 1 - height, 2 - box title
\newtcolorbox{textanswerbox}[2]{%
 width=\textwidth,colback=white,colframe=blue!30!black,floatplacement=H,height=#1,title=#2,clip lower=true,before upper={\parindent0em}}

 \newtcolorbox{eqanswerbox}[1]{%
 width=#1,colback=white,colframe=black,floatplacement=H,height=3em,sharp corners=all,clip lower=true,before upper={\parindent0em}}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answertext}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

%Arguments are 1 - height, 2 - box title, 3 - column definition
 \NewEnviron{answertable}[3]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                        \begin{tabular}{#3}
                                \BODY
                        \end{tabular}
                        \end{table}
        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title, 3 - title, 4- equation label, 5 - equation box width
 \NewEnviron{answerequation}[5]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                \renewcommand{\arraystretch}{0.5}% Tighter

                        \begin{tabular}{#3}
                                #4 =	&
                        \clipbox{0pt 0pt 0pt 0pt}{

                        \begin{eqanswerbox}{#5}
                                $\BODY$
                        \end{eqanswerbox}
                        } \\
                        \end{tabular}
                        \end{table}

        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answerderivation}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

\newcommand{\Checked}{{\LARGE \XBox}}%
\newcommand{\Unchecked}{{\LARGE \Square}}%
\newcommand{\TextRequired}{{\textbf{Place Answer Here}}}%
\newcommand{\EquationRequired}{\textbf{Type Equation Here}}%


\newcommand{\answertextheight}{5cm}
\newcommand{\answertableheight}{4cm}
\newcommand{\answerequationheight}{2.5cm}
\newcommand{\answerderivationheight}{14cm}

\newcounter{QuestionCounter}
\newcounter{SubQuestionCounter}[QuestionCounter]
\setcounter{SubQuestionCounter}{1}

\newcommand{\subquestiontitle}{Question \theQuestionCounter.\theSubQuestionCounter~}
\newcommand{\newquestion}{\stepcounter{QuestionCounter}\setcounter{SubQuestionCounter}{1}\newpage}
\newcommand{\newsubquestion}{\stepcounter{SubQuestionCounter}}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\indices}{indices}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\lstset{language=[LaTeX]TeX,basicstyle=\ttfamily\bf}

\pagestyle{myheadings}
\markboth{Homework 3}{Fall 2021 CS 475/675 Machine Learning: Homework 3}

\title{CS 475 Machine Learning: Homework 3 Analytical \\
(35 points)\\
\Large{Assigned: Friday, September 24, 2021} \\
\Large{Due: Friday, October 8, 2021, 11:59 pm US/Eastern}}
\author{Partner 1: NAME (JHED), Partner 2:  NAME (JHED)}
\date{}

\begin{document}
\maketitle
\thispagestyle{headings}

\section*{Instructions }
We have provided this \LaTeX{} document for turning in this homework. We give you one or more boxes to answer each question.  The question to answer for each box will be noted in the title of the box.  You can change the size of the box if you need more space.\\

{\bf Other than your name, do not type anything outside the boxes. Leave the rest of the document unchanged.}\\


\textbf{
%Do not change any formatting in this document, or we may be unable to
  %grade your work. This includes, but is not limited to, the height of
  %textboxes, font sizes, and the spacing of text and tables.  Additionally,
  Do
  not add text outside of the answer boxes.  You are allowed to make boxes larger if needed.
  % Entering your answers are the only
  %changes allowed.
  }\\


\textbf{We strongly recommend you review your answers in the generated PDF to
  ensure they appear correct. We will grade what appears in the answer boxes in
  the submitted PDF, NOT the original latex file.}

% \section*{ Notation}
% {
% \centering
% \smallskip\begin{tabular}{r l}
% \(\vec{x_i}\) & One input data vector. \(\vec{x_i}\) is \(M\) dimensional.
% \(\vec{x_i} \in \mathbb{R}^{1 \times M}\).  \\ &
% We assume $\vec{x_i}$ is augmented with a  $1$ to include a bias term. \\ \\
% \(\vec{X}\) & 	A matrix of concatenated \(\vec{x_i}\)'s. There are \(N\) input vectors, so \(\vec{X} \in \mathbb{R}^{N \times M}\) \\ \\
% \(y_i\) & The true label for input vector \(\vec{x_i}\). In regression problems, \(y_i\) is continuous. \\ & In general ,\(y_i\) can be a vector, but for now we assume it's a scalar: \(y_i \in \mathbb{R}^1\). \\ \\

% \(\vec{y}\) & 	A vector of concatenated \(y_i\)'s. There are \(N\) input vectors, so \(\vec{y} \in \mathbb{R}^{N \times 1}\) \\ \\

% \(\vec{w}\) & A weight vector. We are trying to learn the elements of \(\vec{w}\). \\
% & \(\vec{w}\) is the same number of elements as \(\vec{x_i}\) because we will end up computing \\
% & the dot product \(\vec{x_i} \cdot \vec{w}\). \\
% & \(\vec{w} \in \mathbb{R}^{M \times 1}\). We assume the bias term is included in \(\vec{w}\). \\ \\

% \(h(\vec(x))\) & The true regression function that describes the data. \\ \\
 
% i.i.d. & Independently and identically distributed. \\ \\

% Bias-variance  & We can write \(E_D[(f(x, D) - h(x))^2]\) = \\
% decomposition  & \((E_D[f(x, D) - h(x))^2 + E_D[(f(x, D) - E_D[f(x, D)])^2]\) \\
%                             & where the first term is the bias squared, and the second term is the variance.\\ \\

%  Notes: & In general, a lowercase letter (not boldface), $a$, indicates a scalar. \\
%   & A boldface lowercase letter, $\vec{a}$, indicates a vector. \\  &  A boldface uppercase letter, $\vec{A}$, indicates a matrix. \\
% \end{tabular}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% -----------------------------------------------------------

\pagebreak
\section{Neural Networks}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{enumerate}

\item Consider a neural network model with $n$ layers, where the vertex for each internal layer is determined by a ReLU activation function applied to a weighted combination of vertices of the previous layer, while the output layer is the given by a weighted linear function of the outputs of the previous layer (without ReLU).

\begin{itemize}

\item[(a)] Show that if we replace all ReLU units by the identity function $f(x) = x$, the resulting neural network yields a linear regression model of the inputs.

\item[(b)] Is there a unique loss minimizer setting of weights for this model?  Explain.

\end{itemize}

% Problem 1.1
\begin{answertext}{20cm}{}

(a) \\
In a neural network, the j-th output at the i-th layer is given by $ x_{ij} = g(\beta_{0} + \vec{\beta^{T}_{i - 1}}\vec{x_{i-1}}) $. \\
In this case, the activation function is $ g(x) = x $. \\
Thus, $ x_{ij} = \beta_{0} + \vec{\beta^{T}_{i - 1}}\vec{x_{i-1}} $. The output from the previous layer will be exactly the input for the next layer. \\
We want to show that if the input of a layer is a linear combination of model inputs, then the output of this layer is also a linear combination of model inputs. \\
Let $ \vec{x_{i-1}} = \vec{\beta_{0,i-1}} + \vec{B_{i-1}}\vec{x_{0}} $, where $ \vec{x_{i-1}} $ is the vector of output from layer i-1,
$ \vec{\beta_{0,i-1}} $ is the vector of biases, $ \vec{B_{i-1}} $ is the matrix whose rows are the linear combination coefficients for each output of the layer,
and $ \vec{x_{0}} $ is the model inputs. \\
Then, for each output $ x_{ij} $ in layer i,
\begin{align*}
x_{ij} &= \beta_{0,ij} + \vec{\beta^{T}_{ij}}\vec{x_{i-1}} \\
&= \beta_{0,ij} + \vec{\beta^{T}_{ij}}(\vec{\beta_{0,i-1}} + \vec{B_{i-1}}\vec{x_{0}}) \\
&= (\beta_{0,ij} + \vec{\beta^{T}_{ij}}\vec{\beta_{0,i-1}}) + (\vec{\beta^{T}_{ij}}\vec{B_{i-1}})\vec{x_{0}}
\end{align*}
Which is a linear combination of $ \vec{x_{0}} $. \\
The whole neural model is but stacking of such layers. Since the each of model inputs is obviously a linear combination of itself, the model output is also a linear combination of the inputs. \\
\\
(b) \\
Assume that we are talking about the model discussed in part (a). \\
For an arbitrary type of loss, assume that we now have a setting of weights that minimizes the loss. If we now multiply all the $ \beta_{0} $ and $ \vec{\beta^{T}_{i-1}} $ in layer i-1 by $\lambda$, and divide all the $ \vec{\beta^{T}_{i}} $ in layer i by $\lambda$, the final output of the model will not change because all the layers are linear combinations of the previous layers. Thus, the modified weights also minimize the loss. Thus, loss minimizer setting of weights is not unique. We can prove this quickly: \\
\\
If $ \vec{x_{i-1}} = \lambda(\vec{\beta_{0,i-1}} + \vec{B_{i-1}}\vec{x_{0}}) $, $ x_{ij} = \beta_{0} + \frac{1}{\lambda}\vec{\beta^{T}_{i - 1}}\vec{x_{i-1}} $
\begin{align*}
x_{ij} &= \beta_{0,ij} + \frac{1}{\lambda}\vec{\beta^{T}_{ij}}\vec{x_{i-1}} \\
&= \beta_{0,ij} + \frac{1}{\lambda}\vec{\beta^{T}_{ij}}\lambda(\vec{\beta_{0,i-1}} + \vec{B_{i-1}}\vec{x_{0}}) \\
&= (\beta_{0,ij} + \vec{\beta^{T}_{ij}}\vec{\beta_{0,i-1}}) + (\vec{\beta^{T}_{ij}}\vec{B_{i-1}})\vec{x_{0}}
\end{align*}
Which does not change.

\end{answertext} 

\pagebreak

\item Consider a neural network with a single intermediate layer and a single output, where the the activation function for every vertex in the intermediate layer is the sigmoid function $\sigma(v) = \frac{1}{1 + \exp\{ - v \}}$ of a weighted linear combination of inputs, while the output is a weighted linear combination of the all the intermediate vertices, and all weights are non-negative.  Show that this model has a likelihood convex with respect to all weight parameters.

Hints:
\begin{itemize}
\item First show that the negative log likelihood for each intermediate layer vertex $v$ (viewed as the outcome), with the weighted combination of inputs for that vertex (viewed as a single feature $x$) is a convex function.  That is, show that:
{
\begin{align*}
h(x) = - v \log \sigma(x) - (1 - v) \log ( 1 - \sigma(x) )
\end{align*}
}
is convex in $x$.  You can do this by showing that the second derivative of this function with respect to $x$ is always non-negative.
\item Then, show that a composition of a linear function $g(w_1, \ldots, w_k)$ of several arguments and a convex function $h(x)$ that is convex in its single argument $x$ is convex in each argument.  That is, show that $h(g(w_1, \ldots, w_k))$ is convex in each $w_1, \ldots, w_k$.
\item Finally, show that a weighted combination of convex functions is also convex, if all weights are non-negative.
\end{itemize}

\begin{answertext}{14cm}{}
We know that the negative log likelihood for a sigmoid function $\sigma(v)$ (which we proved multiple times before) is just:
\begin{align*}
h(x) &= - v \log \sigma(x) - (1 - v) \log ( 1 - \sigma(x) ) \\
&=  - v \log \frac{1}{1 + e^{-x}} - (1 - v) \log ( 1 - \frac{1}{1 + e^{-x}} ) \\
&= v \log (1 + e^{-x}) - (1 - v) (\log e^{-x} - \log (1 + e^{-x}) ) \\
&=  v \log (1 + e^{-x}) - (-x) + v(-x) + \log (1 + e^{-x}) - v \log (1 + e^{-x}) \\
&= x - vx + \log (1 + e^{-x})
\end{align*} 
To prove that it is convex, we need to check its second derivatve to be non-negative.Take the first derivative first:
\begin{align*}
\frac {dh(x)}{dx} &= 1 - v + \frac{1}{1 + e^{-x}}(-1)e^{-x} \\
&= 1 - v -  \frac{e^{-x}}{1 + e^{-x}} \\
&= \frac{1}{1 + e^{-x}} -v
\end{align*}
Then we take the second derivative:
\begin{align*}
\frac {d^2h(x)}{d^2x} &= - (1+e^{-x})^{-2}(-1)e^{-x} \\
&= \frac{e^{-x}}{(1+e^{-x})^2}
\end{align*}
we know that $e^{-x}$ is always positive, so $1+e^{-x}$ is also positive. Thus, both numerator and denominator are positive, so the second derivatve expression is positive, proving convex.
  
\end{answertext} 
\begin{answertext}{22cm}{}
Then, as $x$ is weighed linear combination of inputs, we want to show that $h(x)=h(g(w_1, \ldots, w_k))$ is also convex in each $w_i$. Again, we take first derivative first:
\begin{align*}
\frac{\partial h(g(w_1, \ldots, w_k))}{\partial w_i} & = \frac{dh(g(w_1, \ldots, w_k))}{dg(w_1, \ldots, w_k)}\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i}
\end{align*}
Then take the second derivative:
\begin{align*}
\frac{\partial^2 h(g(w_1, \ldots, w_k))}{\partial^2 w_i}  =& \frac{dh(g(w_1, \ldots, w_k))}{dg(w_1, \ldots, w_k)}\frac{\partial^2 g(w_1, \ldots, w_k)}{\partial^2 w_i} \\
&+\frac{d^2h(g(w_1, \ldots, w_k))}{d^2g(w_1, \ldots, w_k)}\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i}\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i}\\
&= \frac{dh(x)}{dx}\frac{\partial^2g(w_1, \ldots, w_k)}{\partial^2w_i}+\frac{d^2h(x)}{d^2x}(\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i})^2\\
&(\text{where } x = g(w_1, \ldots, w_k))
\end{align*}
we know that $g(w_1, \ldots, w_k)$ is a linear function, so its second derivative is zero: $\frac{\partial^2g(w_1, \ldots, w_k)}{\partial^2w_i} = 0$. We have proved before that $\frac{d^2h(g(w_1, \ldots, w_k))}{d^2g(w_1, \ldots, w_k)}=\frac{d^2h(x)}{d^2x}>0$, and due to the square we also have $(\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i})^2 \geq 0$. So we have:
\begin{align*}
\frac{\partial^2h(g(w_1, \ldots, w_k))}{\partial^2w_i}  =& \frac{d^2h(x)}{d^2x}(\frac{\partial g(w_1, \ldots, w_k)}{\partial w_i})^2 \geq 0
\end{align*}
As the second derivative is non-negative, the function is convex in each of the $w_i$.\\
Finally, the last layer, which is linear combination of all $h(x)$ with non-negative weights, we can write it as $H = \sum_{j=0}^{n}\beta_jh_j(g(w_1, \ldots, w_k))$ where $\beta_j\geq 0$ \\
First take the second derivative with respect to $w_i$:
\begin{align*}
\frac {\partial^2 H}{\partial^2 w_i} = \sum_{j=0}^{n}\beta_j\frac{\partial^2h_j(g(w_1, \ldots, w_k))}{\partial^2 w_i}
\end{align*}
As we have $\beta_j \geq 0$ and $\frac{\partial^2h_j(g(w_1, \ldots, w_k))}{\partial^2 w_i} \geq 0$ as we have proved above, we have $\frac {\partial^2 H}{\partial^2 w_i} \geq 0$ for all $w_i$. Thus, H is convex to all $w_i$ \\
Secondly take the second derivative with respect to $\beta_j$:
\begin{align*}
\frac {\partial^2 H}{\partial^2 \beta_j} =& \frac {\partial }{\partial \beta_j}\frac{\partial^2h_j(g(w_1, \ldots, w_k))}{\partial^2 w_i} \\
=& 0
\end{align*}
As  $\frac {\partial^2 H}{\partial^2 \beta_j} \geq 0$, H is also convex to all $\beta_j$. Thus, we have shown that the likelihood (H) is convex to all weight parameters ($w_i$ and $\beta_j$).
\end{answertext} 
\pagebreak

\item Assume there exist a setting of parameters for this model that minimizes the squared loss, such that at least two intermediate nodes have different weights feeding into their activation function.
Show that there exists more than one setting of parameters that minimizes the squared loss.  (This shows the parameters of this model are not identified).

\begin{answertext}{16cm}{}
The expression of squared loss is $L=(y-h(x))^2$. If we have two sets of parameters that has the same, minimon loss, they must have the same prediction value $h(x)$. Now assume we have two intermediate nodes with different parameters. The nodes are $h_1(x)$ and $h_2(x)$:
\begin{align*}
h_1(x) &= \frac{1}{1 + e^{ - x_1w_1-x_2w_2- \ldots -x_kw_k }} \\
h_2(x) &= \frac{1}{1 + e^{ - x_1v_1-x_2v_2- \ldots -x_kv_k }} \\
&\text{where } w \text{ and } v \text{ are different sets of parameters}
\end{align*}
let their corresponding weights on the last layer be $\beta_1$ and $\beta_2$, the total contribution of those two nodes to $h(x)$ is:
\begin{align*}
\beta_1h_1(x)+\beta_2h_2(x)=\frac{\beta_1}{1 + e^{ - x_1w_1-x_2w_2- \ldots -x_kw_k }} + \frac{\beta_2}{1 + e^{ - x_1v_1-x_2v_2- \ldots -x_kv_k }}
\end{align*}
now assume that we swap each value of $w_i$ with $v_i$, so the new $h_1'$ and $h_2'$ are:
\begin{align*}
h_1'(x) &= \frac{1}{1 + e^{ - x_1v_1-x_2v_2- \ldots -x_kv_k }}\\
h_2'(x) &= \frac{1}{1 + e^{ - x_1w_1-x_2w_2- \ldots -x_kw_k }}
\end{align*}
If we want the output of $h(x)$ to be same as before, we can simply swap $\beta_1$ and $\beta_2$, making $\beta_1'=\beta_2$, $\beta_2'=\beta_1$
Now the new contribution of those two nodes to $h(x)$ is:
\begin{align*}
\beta_1'h_1'(x)+\beta_2'h_2'(x)=\frac{\beta_2}{1 + e^{ - x_1v_1-x_2v_2- \ldots -x_kv_k }}+\frac{\beta_1}{1 + e^{ - x_1w_1-x_2w_2- \ldots -x_kw_k }} 
\end{align*}
This is same as before. So we have two different parameter sets with the same minimized loss. This can be done by simply swap both the first layer weights and the output layer weights of those two intermediate nodes. Thus, there exists more than one setting of parameters that minimizes the squared loss.
\end{answertext} 

\pagebreak

\item
This question contains multiple parts. Please answer all of these parts in large the answer box following the question. (Feel free to increase the box for additional space.)

Consider the 2-layer neural network shown below. There are three input features $x_1$, $x_2$, and $x_3$ which get fed into one activation ($a_1$) from which a hidden value is computed ($h_1$). The non-linearities connecting activations to hidden values are \textbf{rectified linear units} $ReLU$: $h_1$ = $ReLU(a_1)$, with $ReLU(x) = 0$ if $x < 0$, and $ReLU(x) = x$ otherwise. The output unit takes 2 inputs: the hidden value $h_1$ and $x_1$.

\begin{figure}[htbp]
 \centerline{\includegraphics[scale=0.35]{grad_example.png}}
 \label{fig:backprop}
\end{figure}

\begin{enumerate}[(i) ]
    \item Execute forward propagation on this network, writing the appropriate values for $a_1$, $a_2$, $h_1$, $h_2$ and $\hat{y}$ in the figure above.
    \item Give the expression of $\hat{y}$ as a function of $x_1$, $x_2$, $x_3$, $w_1$, $w_2$, $w_3$, $v_1$, $v_2$ and the $ReLU(\cdot)$ function.
    \item The correct class for example $x = [x_1, x_2, x_3] = [-1, -3, -2]$ is $y = -1$. Please run the backpropagation algorithm to minimize the squared error loss $l = \frac{1}{2}(y - \hat{y})^2$ on this single example. Derive the mathematical expression of the gradients of the loss $l$ with respect to weights $w_1$, $w_2$, and $w_3$, and calculate its numerical value.
    \item Indicate how the value of each parameter below changes after the update: does it increase, decrease, or stay the same?
    \item Derive the update rule for parameters $v_1$ and $v_2$ when running the backpropagation algorithm on the same example $x$, with the squared loss $l$ and a step size $\eta = {1}{2}$. \textit{Hint: you will need to (1) derive the appropriate gradient, (2) evaluate the gradient, (3) update your guess for the parameter $\beta_{\text{new}}$ by subtracting the gradient times the step size from the old guess $\beta_{\text{old}}$.}
\end{enumerate}

% Problem 1.4
\begin{answertext}{22cm}{}

(i) \\
$a_{1} = 3$, $h_{1} = 3$, $\hat{y} = 1$ \\
\\
(ii) \\
$\hat{y} = v_{1}x_{1} + v_{2}ReLU(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3})$ \\
\\
(iii) \\
\begin{align*}
\frac{\partial{l}}{\partial{w_{1}}} &= (\hat{y} - y) \frac{\partial{\hat{y}}}{\partial{w_{1}}} \\
&= (\hat{y} - y) \frac{\partial{(v_{1}x_{1} + v_{2}ReLU(a_{1}))}}{\partial{w_{1}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot \frac{\partial{a_{1}}}{\partial{w_{1}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot x_{1} \\
&= (1 - (-1)) \cdot 1 \cdot 1 \cdot (-1) \\
&= -2
\end{align*}
\begin{align*}
\frac{\partial{l}}{\partial{w_{2}}} &= (\hat{y} - y) \frac{\partial{\hat{y}}}{\partial{w_{2}}} \\
&= (\hat{y} - y) \frac{\partial{(v_{1}x_{1} + v_{2}ReLU(a_{1}))}}{\partial{w_{2}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot \frac{\partial{a_{1}}}{\partial{w_{2}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot x_{2} \\
&= (1 - (-1)) \cdot 1 \cdot 1 \cdot (-3) \\
&= -6
\end{align*}
\begin{align*}
\frac{\partial{l}}{\partial{w_{1}}} &= (\hat{y} - y) \frac{\partial{\hat{y}}}{\partial{w_{3}}} \\
&= (\hat{y} - y) \frac{\partial{(v_{1}x_{1} + v_{2}ReLU(a_{1}))}}{\partial{w_{3}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot \frac{\partial{a_{1}}}{\partial{w_{3}}} \\
&= (\hat{y} - y) \cdot v_{2} \frac{\partial{ReLU(a_{1})}}{\partial{a_{1}}} \cdot x_{3} \\
&= (1 - (-1)) \cdot 1 \cdot 1 \cdot (2) \\
&= 4
\end{align*}
  
\end{answertext} 

\begin{answertext}{22cm}{}

(iv) \\ 
$w_{1}$ and $w_{2}$ will increase, while $w_{3}$ will decrease. \\
\\
(v) \\
\begin{align*}
\frac{\partial{l}}{\partial{v_{1}}} &= (\hat{y} - y) \frac{\partial{\hat{y}}}{\partial{v_{1}}} \\
&= (\hat{y} - y) \cdot x_{1} \\
&= (1 - (-1)) \cdot (-1) \\
&= -2
\end{align*}
\begin{align*}
\frac{\partial{l}}{\partial{v_{2}}} &= (\hat{y} - y) \frac{\partial{\hat{y}}}{\partial{v_{2}}} \\
&= (\hat{y} - y) \cdot h_{1} \\
&= (1 - (-1)) \cdot 3 \\
&= 6
\end{align*}
\begin{align*}
v_{1,new} &= v_{1,old} - \eta \frac{\partial{l}}{\partial{v_{1}}} \\
&= 2 - 12 \times (-2) \\
&= 26
\end{align*}
\begin{align*}
v_{2,new} &= v_{2,old} - \eta \frac{\partial{l}}{\partial{v_{2}}} \\
&= 1 - 12 \times 6 \\
&= -71
\end{align*}

\end{answertext}

\end{enumerate}

% -----------------------------------------------------------
\pagebreak
\section{SVMs}

\begin{enumerate}

\item Consider 2D classification problem with 3 classes with the following training sample:\\\\\
\textbf{Class 1:} (0,1), (0,2), (1,1)\\
\textbf{Class 2:} (-2,0), (-1,0), (-1,-1)\\
\textbf{Class 3:} (2,0), (3,0), (2,-1)\\
\begin{enumerate}[(a)]
    \item 
    Explain how you would formulate an SVM model to solve this problem. (Hint: How many classifiers do you need?)\\

% Question 2.1a
\begin{answertext}{5cm}{}  

We will use three support vector machines in this problem, corresponding to the three classes that we want to classify into. Each SVM predicts whether or not the point belongs to the class that this SVM corresponds with. Either it is predicted to belong to this class, or that it belongs to the other two classes.

\end{answertext} 
    \item
    Find $w$, $b$, and corresponding support vectors for each classifier. Please draw the plot.  You can take a screenshot or photo and use the \texttt{{\textbackslash}includegraphics\{\}} directive.\\
\includegraphics[scale=0.25]{p2.2.jpg}
    \item
    Given a test sample, explain how to make a decision.\\

%Question 2.1c
\begin{answertext}{5cm}{}

Given a sample $\vec{x}$, we evaluate $y_{i} = \vec{w_{i}}\vec{x} + b_{i}$ for $i = 1, 2, 3$. We find the greatest $y_{i}$ of the three, and the sample will be classified into the class that this SVM corresponds to.

\end{answertext} 
\end{enumerate}

\end{enumerate}

\end{document}
