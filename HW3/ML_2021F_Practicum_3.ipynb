{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-2021F-Practicum 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMcwWfBvAgZX"
      },
      "source": [
        "# 1. Visualizaing aspects of NN training \n",
        "In this notebook, we’re going to ask you to do 3 types of simple visualizations surrounding the training of your NNs.\n",
        "\n",
        "###1. Data visualization \n",
        "In this step, we’re going to examine our training data. This code is largely written for you, as an example.\n",
        "It’s important to examine your data before you start training your models on it, so that you have a good idea of what kind of data your working on, and what kinds of errors you should look out for.\n",
        "\n",
        "###2. Training Visualization \n",
        "In the second step, we’re going to visualize the training and valida- tion losses of our models as we train them.\n",
        "This is a very important aspect of training deep models. Looking at a graph of these 2 losses can tell you a lot about how well your model is learning what you want it to.\n",
        "\n",
        "###3. Hyperparameter Visualization \n",
        "The last step of visualization is hyperparameter search. There are many, many hyperparameters in a typical DNN. How do we know which values to set\n",
        "for each of these parameters?\n",
        "\n",
        "The short answer is, there’s really no good answer. The best we can do is run the model multiple times on different values, and pick the ones that do the best on our development data.\n",
        "\n",
        "Here, we’ll take turns fixing all of our hyperparameters except one, and examining how our perfor- mance changes as we change that one parameter. Ideally, to be confident in our results, we should get a “U” shape (for scalar parameters), which indicates we’ve examined the full span of which values work for that hyperparameter, when the other values are fixed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50O-3hZKAl3N"
      },
      "source": [
        "##1. Visualizing Our Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFuCNh-wClAA"
      },
      "source": [
        "# imports\n",
        "from collections import defaultdict import numpy as np\n",
        "from data import load\n",
        "import matplotlib.pyplot as plt %matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_QOAXUZCmWG"
      },
      "source": [
        "# Load the data\n",
        "data, labels = load(path='./release-data', split='train') # TODO: Make this point to the correct data directory.\n",
        "dev_data, dev_labels = load(path='./release-data', split='dev') # TODO: Make this point to the correct data directory. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMvf8lKrCt7-"
      },
      "source": [
        "### 1.1 The Label Distribution \n",
        "The first thing we’ll examine is our training set label distribution. We’d like to know what our training set looks like, and how it compares to the distribution of our validation set.\n",
        "If things don’t look very similar, we might have a domain mismatch. Adapting models trained from a different domain that what they’re being evaluated on is a very active area of research, currently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBq0jzQkC54R"
      },
      "source": [
        "counts = defaultdict(int) \n",
        "\n",
        "for label in labels:\n",
        "  counts[label] += 1 plt.figure()\n",
        "\n",
        "plt.title(\"Training set label distribution\") \n",
        "\n",
        "k = counts.keys()\n",
        "v = counts.values()\n",
        "plt.bar(list(k), height=list(v))\n",
        "dev_counts = defaultdict(int) \n",
        "\n",
        "for label in dev_labels:\n",
        "  dev_counts[label] += 1 \n",
        "plt.figure()\n",
        "plt.title(\"Development set label distribution\") \n",
        "dk = dev_counts.keys()\n",
        "dv = dev_counts.values()\n",
        "plt.bar(list(dk), height=list(dv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbTG3yKAEzy-"
      },
      "source": [
        " Luckily, our label distribution looks fairly even across train and dev, so we won’t worry about domain adaptation techniques. We’ll just train our models assuming that the training and test distributions are equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnzsAt8VE5If"
      },
      "source": [
        "###1.2 Visualizing our data\n",
        "Next, let’s visualize some of our data points. Since the dataset we’re working on is a vision dataset, we can actually look at our data as images.\n",
        "Let’s take a look at one example for each label that we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYqsmSybE3GU"
      },
      "source": [
        "# Plot some examples of the data\n",
        "for label in range(10):\n",
        "  for i in range(len(labels)):\n",
        "    if int(labels[i]) == label: \n",
        "      label_idx = i\n",
        "      break\n",
        "\n",
        "plt.figure()\n",
        "plt.title(f\"Label: {label}\")\n",
        "ex = np.array(data[label_idx], dtype=float) \n",
        "plt.imshow(ex.reshape((28,28)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS747BWWFBmL"
      },
      "source": [
        "##2. Visualizing our model’s learning\n",
        "An important aspect of training DNN models is visualizing the training and development loss as our models train, as well as the accuracies.\n",
        "These graphs can tell us a lot about how well our models are doing. For instance, if we see that our training loss is going down, but our dev loss starts going up, we know that we are overfitting and we should have stopped training.\n",
        "The framework we’ve given you for training models logs dev and train loss / accuracy as the model trains. Using these logs, we can take a look into our model’s behavior over time.\n",
        "You might want to use these graphs to help you debug your models as well, while you’re developing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEN0N0ZZE99T"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt %matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xlx9Ac_FMnQ"
      },
      "source": [
        "###2.1 Feed Forward Model\n",
        "The first model we’ll take a look at is our simple feedforward model. Before you run the next cell, you should have implemented and trained a preliminary version of this model, and it’s logs should be stored somewhere.\n",
        "You’ll need to point to those logs to graph the loss and accuracies!\n",
        "\n",
        "Be sure to plot the losses of your best model (The model you’re submitting) here before you turn the notebook in!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAF-drVYCs20"
      },
      "source": [
        "ff_metrics = pd.read_csv('./best/ff.csv') # TODO: point this to the correct log file!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE5QNWalFSt-"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"FF Model Train and Dev Loss\")\n",
        "ax = plt.gca() \n",
        "ff_metrics.plot(kind='line',x='step',y='train_loss',ax=ax) \n",
        "ff_metrics.plot(kind='line',x='step',y='dev_loss', color='red', ax=ax) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db5ce-KIFXIB"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"FF Model Train and Dev Accuracy\")\n",
        "### TODO: Plot FF Model Train and Dev Accuracy\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB0CciqvGPnV"
      },
      "source": [
        "Does the loss graph tell you anything? Do you need to train for more steps? Should you train for less? These kinds of plots can tell you a lot about your model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBrTRVNyGSsa"
      },
      "source": [
        "###2.2 Basic CNN Model\n",
        "Now, let’s to the same for the cnn model.\n",
        "This model still needs to be implemented! Make sure you have finished the coding portion for the CNN model, and have trained the model before you plot things.\n",
        "Does the plot look correct? It might tell you something about which hyperparameters you want to change (learning rate, batch size, number of steps) from the Feedforward Model setting! You might notice that this graph is quite a bit smoother than the ff model. This model takes many more steps to train than the FF model!\n",
        "Be sure to plot the losses of your best cnn model (The version you’re submitting) here before you turn the notebook in!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V-OKcU8GWK_"
      },
      "source": [
        "cnn_metrics = pd.read_csv('./best/cnn.csv') # TODO: point this to the correct log file!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l2kFEEuGaih"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"CNN Model Train and Dev Loss\")\n",
        "### TODO: CNN Model Train and Dev Loss\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0PSX96qGPJZ"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"CNN Model Train and Dev Accuracy\")\n",
        "### TODO: CNN Model Train and Dev Accuracy\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsv8rxZOG7Eh"
      },
      "source": [
        "###2.3 Best Model\n",
        "Now, do the same for the best model (The model you create from scratch!)\n",
        "\n",
        "Again, you might find it helpful to use this code to examine your model as you debug it and try to boost it’s performance.\n",
        "\n",
        "Be sure to plot the losses of your best model (The model you’re submitting) here before you turn the notebook in!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_BVfnMmG_x0"
      },
      "source": [
        "best_metrics = pd.read_csv('./best/best.csv') # TODO: point this to the correct log file!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y2Jsp_wHDYC"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"Best Model Train and Dev Loss\")\n",
        "### TODO: Best Model Train and Dev Loss\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0eOKihHO-G"
      },
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"Best Model Train and Dev Accuracy\")\n",
        "### TODO: Best Model Train and Dev Accuracy \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzJR2WCIHZSa"
      },
      "source": [
        "##3. Visualizing Hyperparameter Search\n",
        "You might have noticed that these models, especially the more complex ones, have tons of hyper- parameters!\n",
        "\n",
        "In this next section of visualization, we’re going to visualize the effects of specific hyperparameters. Namely, we’re going to be performing a hyperparameter sweep over certain hyperparameters.\n",
        "\n",
        "To do this, you should freeze all hyperparameters except one. We will then choose a range of values for the one unfrozen hyperparameter. We’ll train an individual model for each value of the range, and then store it’s performance on held out development data.\n",
        "\n",
        "Once we have dev accuracy for each setting, we can plot it and examine the learning trends as we sweep over that hyperparameters.\n",
        "\n",
        "Ideally, if we have chosen an effective range, we should see something of an upside-down “U” shape, indicating that we have pushed the hyperparameter to either extreme where it starts to hurt performance, and we might have found something of an optimal value.\n",
        "\n",
        "Of course, this upside-down “U” and it’s maximum value might only be true in the setting where all of our other hyperparameters are frozen. If we change those, then the sweep we just did might not be correct anymore. However, if we’ve chosen good values to freeze our other hyperparameters with, then we should learn something about the hyperparameter we’re examining.\n",
        "\n",
        "Note: You will be training a lot of models in this section. If you set up the sweeps correctly, running them should take a while. You might want to leave some time to run these experiments (go grab lunch or a coffee after you kick them off).\n",
        "\n",
        "You can do this part a number of ways. The easiest way is write a script that runs main.py multiple times and passes in a different value for the hyperparameter in the command line arguments. Then you can manually grab look at performance of each model from it’s output, and plot that in this notebook. This also allows you to save each model in a different spot, so you always have the best model after a sweep. This is nice, because now you don’t have to retrain the model to run it on test data!\n",
        "\n",
        "Another way is to build a training loop in the cells below, and just run through that loop for each value of the hyperparameter. This might be a bit more work, but now you can directly store the dev accuracy of each run in the notebook, which migth make it easier to plot. It’s up to you!\n",
        "\n",
        "Also, please label your plots, so we know what we’re looking at! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu-y6iYBJ7wD"
      },
      "source": [
        "###3.1 Feed Forward Learning Rate Sweep.\n",
        "\n",
        "Let’s examine the effect of increasing and decreasing the initial learning rate. Train a model for each learning rate value, and store the dev accuracy for each, then plot them below.\n",
        "\n",
        "Make sure that you freeze all other hyperparameters of the model to something reasonable, so that the trends you observe are informative!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nktPuIkqHjJP"
      },
      "source": [
        "# e.g. learning_rates = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
        "simple_ff_metrics = pd.read_csv('./3.1/results.csv')\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"FF Model Learning Rate Sweep\")\n",
        "ax = plt.gca()\n",
        "# learning rate represented as 10^(-x) on the x-axis simple_ff_metrics.plot(kind='line',x='learning_rate',y='dev_acc', ax=ax) plt.show()\n",
        "# Optimal Learning rate is 10^(-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBqXrcoLHnS5"
      },
      "source": [
        "###3.2 CNN Number of Channels Sweep\n",
        "\n",
        "Now, let’s take a look at how choice of channel size for conv1 affects the CNN model we’ve im- plemented. Once again, freeze the other hyperparameters, and train a new CNN model for each channel size that you think should be swept over.\n",
        "\n",
        "The CNN Model should take a while to train, so you don’t pick too dense of a set. Additionally, the larger this value is, the longer the model takes to train. This is another tradeoff we need to keep in mind. We don’t want to select a value that takes too long to train! So it’s okay for us to examine hyperparameters that range from too small to learn to too large to train efficiently, even though we won’t get the “U-shape” we desire!\n",
        "Plot the results of dev accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxUaQQHXHrBS"
      },
      "source": [
        "# e.g. number_of_channels = [10, 30, 50, 70, 90, 110, 130]\n",
        "simple_cnn_metrics = pd.read_csv('./3.2/simple-cnn-result.csv')\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.title(\"Simple CNN Model Conv1 Channel Size Sweep\")\n",
        "ax = plt.gca() \n",
        "simple_cnn_metrics.plot(kind='line',x='num_channels',y='dev_acc', ax=ax) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH2NW6DcH4VV"
      },
      "source": [
        "###3.3 Best Model Sweep\n",
        "\n",
        "Let’s also examine the effect of some hyperparameters of the model you created for this task!\n",
        "\n",
        "Pick two hyperparameters in your model and perform separate sweeps on them. You should plot at least two graphs below! You can pick any hyperparameters you’d like. Try to pick ones that you are the most curious about, and examine the behavior.\n",
        "\n",
        "Can you learn anything new about these hyperparameters, and what valid values for them are?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9XvwUXKAmI"
      },
      "source": [
        "###3.3.1 Hyperparameter 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClN-hmelH8D-"
      },
      "source": [
        " # e.g. number_of_linear_features_in_1st_fully_connected_layer = [10, 30, 50, 70, 90, 110, 130]\n",
        "best_linear_metrics = pd.read_csv('./3.3/best-linear-features-result.csv') \n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.title(\"Best Model FC1 Linear Feature Size Sweep\")\n",
        "ax = plt.gca() \n",
        "best_linear_metrics.plot(kind='line',x='num_linear-features',y='dev_acc', ax=ax) \n",
        "plt.show()\n",
        "# Valid range between 10 and infinity\n",
        "# The larger the feature size, the better until the number of linear-features is larger than the\n",
        "# number of inputs into the linear layer. There is also a large time tradeoff."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErGx9eXFIFTa"
      },
      "source": [
        "###3.3.2 Hyperparameter 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrMPF_0cID7C"
      },
      "source": [
        "# 2nd_Max_Pooling_Layer_Kernel_size = [1, 2, 3, 4, 5, 6]\n",
        "best_maxpool_metrics = pd.read_csv('./3.3/best-pool2-result.csv')\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.title(\"Best Model Second Max Pooling Layer Kernel Size Sweep\") \n",
        "ax = plt.gca() \n",
        "best_maxpool_metrics.plot(kind='line',x='maxpool',y='dev_acc', ax=ax) \n",
        "plt.show()\n",
        "# Valid range between 1 and 6\n",
        "# The smaller than Max Pooling Kernel, the better the development accuracy\n",
        "# However, this might not mean that the model will perform better on the test set\n",
        "# since the model could be just overfitting on the training and development sets"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}