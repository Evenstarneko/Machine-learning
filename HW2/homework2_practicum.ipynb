{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "homework2_practicum.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwU-lXZEFV-4"
      },
      "source": [
        "# Homework 2: Practicum\n",
        "### 15 points total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnRCMdPrSBw8"
      },
      "source": [
        "Chang Yan (cyan13@jhu.edu), Jingguo Liang (jliang35@jhu.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS_0juBfFV-5"
      },
      "source": [
        "**Instructions:**\n",
        "This notebook is intended to guide you through creating and exploring your dataset. Please answer all questions in this notebook (you will see <font color='blue'>TODO</font> annotations for where to include your answers). At the beginning of each part, we will bullet the expected deliverables for you to complete. All questions can be answered in 1-4 sentences, unless otherwise noted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKB_9fxJFV-5"
      },
      "source": [
        "## Part 1: Choosing a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-H02HwkFV-6"
      },
      "source": [
        "Pick a dataset that you like (may be the same as hw1), but it should be within the **supervised learning** paradigm. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i70yHkXFV-_"
      },
      "source": [
        "#### 1) List the source of your dataset along with (very briefly) what you obtained from it.\n",
        "\n",
        "For example: <br /> Obtained features: 28*28 images from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz. <br /> Obtained labels from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWYNsEn3FV_A"
      },
      "source": [
        "<font color='blue'>\n",
        "    My dataset is the same as homework 1, which is the MNIST dataset. I obtained 60000 28*28 images files of hand-written digits from them (They are in one binary file and I need to extract them). There is also another file containing the labels which is also a binary file I need to decode.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xudIDiZ0FV_A"
      },
      "source": [
        "## Part 2: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnloVU9iFV_J"
      },
      "source": [
        "If your data is not numerical, this will be difficult for an algorithm to learn directly. So, now that you've seen what the raw data looks like, you will start extracting *numerical* features from the raw data.\n",
        "<br /><br />\n",
        "We obtain features through a process called **feature engineering**. Features may be derived from the existing raw data or may come from other data sources that can be associated with each example. This is a challenging task that often requires domain knowledge about the problem you are trying to solve. \n",
        "<br /><br />\n",
        "For this question, **you will need to add a new feature to your dataset**. You will need some features for the other steps, but these can be very simple and don't need to rely on domain knowledge.\n",
        "<br /><br />\n",
        "If your data is Wikipedia documents, possible features could be number of sentences, word count, the words that appear in the article, number of document revisions, number of contributing authors, number of references, etc. Notice that some of these features could be derived from the raw data (i.e. the words) while others may need to be downloaded separately (i.e. page metadata). If your data are cat images, your features could be focus measure (i.e. blurriness/sharpness) using OpenCV Variance of Laplacian, whether image is grayscale, number of pixels, the pixel color values, etc. You can also use interaction terms, higher order terms, or indicators for ranges as your new feature. \n",
        "<br /><br />\n",
        "You are free to obtain features in any way you like as long as you can justify why the features your propose should help solve the problem you've defined.\n",
        "<br /><br />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQx_bNBUIkDH",
        "outputId": "c211a64f-20a4-4957-d64a-1a9b53201f81"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf0HOYw8FV_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923ec1e0-18bc-4ce2-d6fa-90cef691b0e8"
      },
      "source": [
        "#######################################################\n",
        "# First read in the data as before\n",
        "import cv2\n",
        "import numpy as np\n",
        "f = open('/content/drive/MyDrive/train-labels.idx1-ubyte', 'rb') # opening the label binary file\n",
        "content = f.read() # reading all lines\n",
        "# the labels start at positon 8 and has total of 60000 ones\n",
        "y = np.zeros(60000)\n",
        "for i in range(60000):\n",
        "  y[i] = content[i+8]\n",
        "f = open('/content/drive/MyDrive/train-images.idx3-ubyte', 'rb') # opening the data binary file\n",
        "content = f.read() # reading all lines\n",
        "# the images start at postion 16 and each is 28*28\n",
        "X = np.zeros((60000, 28, 28))\n",
        "for i in range(60000):\n",
        "  temp = content[i*28*28+16: (i+1)*28*28+16]\n",
        "  array = np.zeros(28*28)\n",
        "  for j in range(28*28):\n",
        "    array[j] = temp[j]\n",
        "  X[i] = np.reshape(array,(28,28))\n",
        "\n",
        "# compute new feature\n",
        "X_flat = np.reshape(X, (60000, 28*28))\n",
        "# feature 1: average intensity\n",
        "feature1 = np.mean(X_flat, axis = 1)\n",
        "feature1 = np.reshape(feature1, (60000, 1))\n",
        "# feature 2: standard deviation of intensity\n",
        "feature2 = np.std(X_flat, axis = 1)\n",
        "feature2 = np.reshape(feature2, (60000, 1))\n",
        "# feature 3: edge dectction of each picture\n",
        "feature3 = np.zeros((60000, 28, 28))\n",
        "ratio = 3 # use openCV recommendation\n",
        "kernel_size = 3\n",
        "low_threshold = 50\n",
        "for i in range(X.shape[0]):\n",
        "  X_blur = cv2.blur(np.uint8(X[i]), (3,3)) # follow openCV guidelines to blur first\n",
        "  feature3[i] = cv2.Canny(X_blur, low_threshold, low_threshold*ratio, kernel_size)\n",
        "feature3 = np.reshape(feature3, (60000, 28*28))\n",
        "\n",
        "# Convert X and y to numpy arrays with appropriate dimensions\n",
        "# stack all features\n",
        "X = np.append(X_flat, feature1, axis = 1)\n",
        "X = np.append(X, feature2, axis = 1)\n",
        "X = np.append(X, feature3, axis = 1) \n",
        "y = y\n",
        "#######################################################\n",
        "# check the shape\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 1570)\n",
            "(60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvDukQY_MOT"
      },
      "source": [
        "#### 2) Describe the new features in your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp6_xtTFV_J"
      },
      "source": [
        "<font color='blue'>\n",
        "    I added 3 sets of features (a total of 1+1+28*28 = 786 features). The original features I used in last homework was only the image intensity, which is 28*28 = 784 features. This time I added 786 new features, so now it has 1570 features in total. The first set of feature is the mean intensity of each image. This is useful because this describes the overall intensity of each image, and images with more dark pixels will have higher average intensity. The second set of feature is the standard deviation of intensity of each image, this is useful beacuse it describes the level of dispersion in intensity of each image. The third set of features is the edge detection result of OpenCV Canny function. This is actually consists of 784 features because the edge detection result is a 28*28 1/0 binary array, so we need 784 features to describe the full distribution of edges. This set of features is useful beacuse edge detection is such a commonly used way in image preprocessing to extract the boundaries of objects in the picture.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qo5qk2ikX05"
      },
      "source": [
        "## Part 3: Evaluation: Usefulness of Added Feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWLu3_dfKATF"
      },
      "source": [
        "Now that you have added a new feature, train 2 logistic regression classifiers, one with the new feature and 1 without the new feature. \n",
        "\n",
        "Choose at least 3 metrics that you have seen (ex. from Practicum 1) to evaluate and compare the performance of your 2 models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0UNJXD2lpFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdb65b6-75d7-49a2-dc05-69603e9e3613"
      },
      "source": [
        "#######################################################\n",
        "# TODO: train 2 logistic regression classifiers. You may use libraries like sklearn for training. \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# train_test split of two sets of features\n",
        "# This is the set with new features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "print(f'Training examples: {X_train.shape[0]}\\nTesting examples: {X_test.shape[0]}')\n",
        "# This is the set without new features\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_flat, y, test_size=0.25, random_state=0)\n",
        "print(f'Training examples: {X_train.shape[0]}\\nTesting examples: {X_test.shape[0]}')\n",
        "# model 1 with new features\n",
        "clf = LogisticRegression(random_state=0, max_iter = 1000, class_weight = 'balanced').fit(X_train, y_train)\n",
        "y_test_hat = clf.predict(X_test)\n",
        "# model 2 without new features\n",
        "clf2 = LogisticRegression(random_state=0, max_iter = 1000, class_weight = 'balanced').fit(X_train_2, y_train_2)\n",
        "y_test_2_hat = clf2.predict(X_test_2)\n",
        "#######################################################"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 45000\n",
            "Testing examples: 15000\n",
            "Training examples: 45000\n",
            "Testing examples: 15000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEO_tRuykiqk",
        "outputId": "98d4b1aa-d655-45ce-a978-cd52dfe97501"
      },
      "source": [
        "# evaluation of models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# with new features\n",
        "# Use binary label to evaluate: label 1 is still 1, other lables are set to 0\n",
        "# So the positive is detecting an \"1\" in image, the negative is all other numbers\n",
        "y_test[y_test != 1] = 0\n",
        "y_test_hat[y_test_hat != 1] = 0\n",
        "y_test_2[y_test_2 != 1] = 0\n",
        "y_test_2_hat[y_test_2_hat != 1] = 0\n",
        "print(\"with new features:\")\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_test_hat).ravel()\n",
        "print(\"TP = \"+str(TP))\n",
        "print(\"FP = \"+str(FP))\n",
        "print(\"FN = \"+str(FN))\n",
        "print(\"TN = \"+str(TN))\n",
        "Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "Precision = Sensitivity = TP/(TP + FP)\n",
        "Specificity = TN/(TN + FP)\n",
        "False_Positive_Rate = FP/(FP + TN)\n",
        "print(\"Accuracy = \"+str(Accuracy))\n",
        "print(\"Precision = \"+str(Precision))\n",
        "print(\"Specificity = \"+str(Specificity))\n",
        "print(\"False Positive Rate = \"+str(False_Positive_Rate))\n",
        "# without new features\n",
        "print(\"without new features:\")\n",
        "y_test_2_binary = y_test_2 # binarize \n",
        "y_test_2_binary[y_test_2 != 1] = 0\n",
        "y_test_pred_2_binary = y_test_2_hat # binarize \n",
        "y_test_pred_2_binary[y_test_2_hat != 1] = 0\n",
        "[TN2, FP2, FN2, TP2] = confusion_matrix(y_test_2, y_test_2_hat).ravel()\n",
        "print(\"TP = \"+str(TP2))\n",
        "print(\"FP = \"+str(FP2))\n",
        "print(\"FN = \"+str(FN2))\n",
        "print(\"TN = \"+str(TN2))\n",
        "Accuracy2 = (TP2 + TN2)/(TP2 + TN2 + FP2 + FN2)\n",
        "Precision2 = Sensitivity2 = TP2/(TP2 + FP2)\n",
        "Specificity2 = TN2/(TN2 + FP2)\n",
        "False_Positive_Rate2 = FP2/(FP2 + TN2)\n",
        "print(\"Accuracy = \"+str(Accuracy2))\n",
        "print(\"Precision = \"+str(Precision2))\n",
        "print(\"Specificity = \"+str(Specificity2))\n",
        "print(\"False Positive Rate = \"+str(False_Positive_Rate2))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with new features:\n",
            "TP = 1674\n",
            "FP = 86\n",
            "FN = 54\n",
            "TN = 13186\n",
            "Accuracy = 0.9906666666666667\n",
            "Precision = 0.9511363636363637\n",
            "Specificity = 0.9935201928872816\n",
            "False Positive Rate = 0.006479807112718505\n",
            "without new features:\n",
            "TP = 1693\n",
            "FP = 184\n",
            "FN = 35\n",
            "TN = 13088\n",
            "Accuracy = 0.9854\n",
            "Precision = 0.9019712306872669\n",
            "Specificity = 0.9861362266425557\n",
            "False Positive Rate = 0.013863773357444244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25dhjPMDlR3d"
      },
      "source": [
        "#### 3) Was your new feature helpful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzotBuA4mgwu"
      },
      "source": [
        "<font color='blue'>\n",
        "    From the metrics above we can see that the new features are helpful. The accuracy increased from 0.9854 to 0.9907, the precision increased from 0.9020 to 0.9511, and the specificity increased from 0.9861 to 0.9935. The false positive rate decreased from 0.0139 to 0.0065, which is also an improvement. All of the four metrics above indicate that adding the new features are helpful. If we look at the TP, FP, FN and TN themselves, the false positive dramatically decreased, but the false negative increased a little. The true negative also increased largely, but the true positive decreased a little. Overall, there is still more improvement.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WveTotfQmA-l"
      },
      "source": [
        "## Part 4: Evaluation: With vs. Without Regularization\n",
        "\n",
        "Come back to this section after you finish your programming assignment. Evaluate your logistic regression models (with and without regularization) from the programming assignment. Evaluate the models using the validation data we provided and compare the performance of the 2 models. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA3qxvhpmdIE"
      },
      "source": [
        "<font color='blue'>\n",
        "    The first model without regularization get an accuracy of 0.83333(25/30), and converges at a log loss of about 0.465. The second model with regularization and λ = 10 get an accuracy of 0.90000(27/30), and converges at a log loss of about 0.484. They converge at roughly the same number of iterations. If we set a larger λ, the second model will converges slower but the accuracy does not increase. So we can conclude that the second model with regularization works better (has higher accuracy) than the first model, and it works best at λ=10 without affecting the number of iterations much.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isE0mABImiH6"
      },
      "source": [
        "#### 4) Which model out of the two would you choose to deploy in real life?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jUgh46Zmpp8"
      },
      "source": [
        "<font color='blue'>\n",
        "    I would choose to use the model with regularization, as it obviously improves accuracy and did not require much longer time to compute.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll5FhwgKx4K6"
      },
      "source": [
        "## Submit\n",
        "Great work! You're all done.\n",
        "\n",
        "Make sure to submit this Python notebook (as a PDF) and the dataset you created as a zip file. See the homework writeup for directions."
      ]
    }
  ]
}